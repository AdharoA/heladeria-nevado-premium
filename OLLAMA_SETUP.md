# Configuraci√≥n de ADARA con Ollama

## Requisitos Previos

1. **Ollama Instalado**: Descarga desde [ollama.ai](https://ollama.ai)
2. **Modelo deepseek-r1:8b**: Se descargar√° autom√°ticamente

## Pasos de Instalaci√≥n

### 1. Instalar Ollama

**Windows/Mac/Linux:**
- Descarga desde [ollama.ai](https://ollama.ai)
- Sigue las instrucciones del instalador

### 2. Descargar el Modelo deepseek-r1:8b

Abre una terminal y ejecuta:

```bash
ollama pull deepseek-r1:8b
```

Esto descargar√° el modelo (~8GB). Puede tomar varios minutos dependiendo de tu conexi√≥n.

### 3. Verificar que Ollama est√° Ejecut√°ndose

Ollama deber√≠a ejecutarse autom√°ticamente en `http://localhost:11434`

Para verificar:
```bash
curl http://localhost:11434/api/tags
```

Deber√≠as ver una respuesta JSON con los modelos disponibles.

### 4. Configurar Variables de Entorno (Opcional)

Si Ollama est√° en un servidor diferente, configura:

```bash
# En Windows (PowerShell)
$env:OLLAMA_API_URL = "http://tu-servidor:11434"
$env:OLLAMA_MODEL = "deepseek-r1:8b"

# En Linux/Mac
export OLLAMA_API_URL="http://tu-servidor:11434"
export OLLAMA_MODEL="deepseek-r1:8b"
```

### 5. Ejecutar la Aplicaci√≥n

```bash
# Terminal 1: Backend
cd backend
pnpm install
pnpm db:push
pnpm dev

# Terminal 2: Frontend
cd frontend
pnpm install
pnpm dev
```

## Soluci√≥n de Problemas

### Ollama no responde
- Aseg√∫rate de que Ollama est√° ejecut√°ndose: `ollama serve`
- Verifica que est√° en `http://localhost:11434`

### Modelo no encontrado
- Ejecuta: `ollama pull deepseek-r1:8b`
- Verifica con: `ollama list`

### Respuestas lentas
- El modelo deepseek-r1:8b requiere recursos. Aseg√∫rate de tener:
  - 8GB RAM m√≠nimo
  - GPU recomendada (NVIDIA/AMD)

### ADARA no responde
- Abre la consola del navegador (F12) y busca errores
- Verifica que el backend est√° corriendo en puerto 3000
- Comprueba que Ollama est√° disponible

## Caracter√≠sticas de ADARA

‚ú® **ADARA (Amiga Digital de Atenci√≥n y Recomendaci√≥n Avanzada)**

- üí¨ Chat contextual y conversacional
- üç¶ Recomendaciones de helados personalizadas
- üì¶ Ayuda con pedidos y compras
- üéØ Respuestas r√°pidas y precisas
- üåô Tema claro/oscuro autom√°tico
- üì± Interface tipo Messenger compacta

## Modelos Alternativos

Si prefieres usar otro modelo, puedes cambiar en `backend/services/ollama.ts`:

```typescript
const OLLAMA_MODEL = "mistral:7b"; // o cualquier otro modelo
```

Modelos recomendados:
- `deepseek-r1:8b` (Recomendado - Mejor calidad)
- `mistral:7b` (M√°s r√°pido)
- `neural-chat:7b` (Optimizado para chat)
- `llama2:7b` (Alternativa popular)

## Monitoreo

Para ver el uso de recursos de Ollama:

```bash
# Ver modelos cargados
ollama list

# Ver historial
ollama show deepseek-r1:8b
```

## Rendimiento

Tiempos esperados de respuesta (primera vez):
- Primera consulta: 30-60 segundos (cargando modelo)
- Consultas siguientes: 5-15 segundos

Para mejorar rendimiento:
- Usa GPU (NVIDIA CUDA o AMD ROCm)
- Aumenta RAM disponible
- Cierra otras aplicaciones pesadas
